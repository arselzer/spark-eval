{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d98322b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "import string\n",
    "import pathlib\n",
    "import random\n",
    "import threading\n",
    "import time\n",
    "from urllib.parse import urlsplit, urlunsplit\n",
    "import requests\n",
    "import json\n",
    "from py4j.protocol import Py4JJavaError, Py4JError\n",
    "import glob\n",
    "import psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9a04030-0a23-45df-8e70-6a296a4f582d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global configuration\n",
    "SPARK_MEMORY = 50\n",
    "SPARK_CORES = 8\n",
    "DBHOST = 'postgres'\n",
    "QUERY_TIMEOUT = 60 * 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7397f3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spark():\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"app\") \\\n",
    "        .master(f'local[{SPARK_CORES}]') \\\n",
    "        .config(\"spark.driver.memory\", f'{SPARK_MEMORY}g') \\\n",
    "        .config(\"spark.executor.memory\", f'{SPARK_MEMORY}g') \\\n",
    "        .config(\"spark.memory.offHeap.enabled\",False) \\\n",
    "        .config(\"spark.jars\", \"postgresql-42.3.3.jar\") \\\n",
    "        .getOrCreate()\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4170360b-134b-4619-915b-391e877bc203",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_metrics(spark, group_id):\n",
    "    parsed = list(urlsplit(spark.sparkContext.uiWebUrl))\n",
    "    host_port = parsed[1]\n",
    "    parsed[1] = 'localhost' + host_port[host_port.find(':'):]\n",
    "    API_URL = f'{urlunsplit(parsed)}/api/v1'\n",
    "\n",
    "    app_id = spark.sparkContext.applicationId\n",
    "    sql_queries = requests.get(API_URL + f'/applications/{app_id}/sql', params={'length': '100000'}).json()\n",
    "    query_ids = [q['id'] for q in sql_queries if q['description'] == group_id]\n",
    "    if (len(query_ids) == 0):\n",
    "        print(f'query with group {group_id} not found')\n",
    "        return None\n",
    "    query_id = query_ids[0]\n",
    "    print(f'query id: {query_id}')\n",
    "    \n",
    "    query_details = requests.get(API_URL + f'/applications/{app_id}/sql/{query_id}',\n",
    "                                 params={'details': 'true', 'planDescription': 'true'}).json()\n",
    "    \n",
    "    success_job_ids = query_details['successJobIds']\n",
    "    running_job_ids = query_details['runningJobIds']\n",
    "    failed_job_ids = query_details['failedJobIds']\n",
    "    \n",
    "    job_ids = success_job_ids + running_job_ids + failed_job_ids\n",
    "    \n",
    "    job_details = [requests.get(API_URL + f'/applications/{app_id}/jobs/{jid}').json() for jid in job_ids]\n",
    "    \n",
    "    job_stages = {}\n",
    "    \n",
    "    for j in job_details:\n",
    "        stage_ids = j['stageIds']\n",
    "        \n",
    "        stage_params = {'details': 'true', 'withSummaries': 'true'}\n",
    "        stages = [requests.get(API_URL + f'/applications/{app_id}/stages/{sid}', stage_params) for sid in stage_ids]\n",
    "        \n",
    "        job_stages[j['jobId']] = [stage.json() for stage in stages if stage.status_code == 200] # can be 404\n",
    "    \n",
    "    return query_details, job_details, job_stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acbbca1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_db(spark, dbname):\n",
    "    \n",
    "    username = dbname\n",
    "    password = dbname\n",
    "    dbname = dbname\n",
    "\n",
    "    df_tables = spark.read.format(\"jdbc\") \\\n",
    "    .option(\"url\", f'jdbc:postgresql://{DBHOST}:5432/{dbname}') \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .option(\"dbtable\", \"information_schema.tables\") \\\n",
    "    .option(\"user\", username) \\\n",
    "    .option(\"password\", password) \\\n",
    "    .load()\n",
    "\n",
    "    for idx, row in df_tables.toPandas().iterrows():\n",
    "        if row.table_schema == 'public':\n",
    "            table_name = row.table_name\n",
    "            df = spark.read.format(\"jdbc\") \\\n",
    "                .option(\"url\", f'jdbc:postgresql://{DBHOST}:5432/{dbname}') \\\n",
    "                .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "                .option(\"dbtable\", table_name) \\\n",
    "                .option(\"user\", username) \\\n",
    "                .option(\"password\", password) \\\n",
    "                .load()\n",
    "    \n",
    "            print(table_name)\n",
    "            #print(df.show())\n",
    "            df.createOrReplaceTempView(table_name)\n",
    "\n",
    "def random_str(size=16, chars=string.ascii_uppercase + string.digits):\n",
    "    return ''.join(random.choice(chars) for _ in range(size))\n",
    "\n",
    "def set_group_id(spark):\n",
    "    group_id = random_str()\n",
    "    spark.sparkContext.setJobGroup(group_id, group_id)\n",
    "    return group_id\n",
    "\n",
    "def cancel_query(spark, seconds, group_id):\n",
    "    time.sleep(seconds)\n",
    "    print(\"cancelling jobs with id \" + group_id)\n",
    "    print(spark.sparkContext.cancelJobGroup(group_id))\n",
    "    print(\"cancelled job\")\n",
    "\n",
    "def cancel_query_after(spark, seconds):\n",
    "    group_id = random_str()\n",
    "    spark.sparkContext.setJobGroup(group_id, group_id)\n",
    "    threading.Thread(target=cancel_query, args=(spark, seconds, group_id,)).start()\n",
    "    return group_id\n",
    "    \n",
    "def run_query(spark, file):\n",
    "    with open(file, 'r') as f:\n",
    "        query = '\\n'.join(filter(lambda line: not line.startswith('limit') and not line.startswith('-'), f.readlines()))\n",
    "        \n",
    "        print(\"running query: \\n\" + query)\n",
    "        return spark.sql(query)\n",
    "\n",
    "def get_resource_usage(t):\n",
    "    return {\n",
    "        'time': t,\n",
    "        'memory': psutil.virtual_memory(),\n",
    "        'cpu': psutil.cpu_percent(interval=None, percpu=True),\n",
    "        'cpu_total': psutil.cpu_percent(interval=None, percpu=False)\n",
    "    }\n",
    "def explain_str(df):\n",
    "    return df._sc._jvm.PythonSQLUtils.explainString(df._jdf.queryExecution(), 'extended')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ead489e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "resource_usage = []\n",
    "\n",
    "def measure_resource_usage(resource_usage):\n",
    "    t = threading.current_thread()\n",
    "    secs = 0\n",
    "    while getattr(t, \"do_run\", True):\n",
    "        resource_usage.append(get_resource_usage(secs))\n",
    "        #print(\"resource usage: \" + str(resource_usage))\n",
    "        secs += 1\n",
    "        time.sleep(1)\n",
    "\n",
    "def benchmark_query(spark, query, respath, run):\n",
    "    spark.sparkContext._jvm.System.gc()\n",
    "    start_time = time.time()\n",
    "\n",
    "    resource_usage = []\n",
    "\n",
    "    measure_thread = threading.Thread(target=measure_resource_usage, args=(resource_usage, ))\n",
    "    measure_thread.start()\n",
    "\n",
    "    group_id = cancel_query_after(spark, QUERY_TIMEOUT)\n",
    "    df1 = run_query(spark, query)\n",
    "    df1.show()\n",
    "\n",
    "    measure_thread.do_run = False\n",
    "\n",
    "    end_time = time.time()\n",
    "    diff_time = end_time - start_time\n",
    "\n",
    "    execution, jobs, job_stages = extract_metrics(spark, group_id)\n",
    "\n",
    "    with open(respath + f'/resource-usage-{run}.json', 'w') as f:\n",
    "        f.write(json.dumps(resource_usage, indent=2))\n",
    "    with open(respath + f'/explain-{run}.txt', 'w') as f:\n",
    "        f.write(explain_str(df1))\n",
    "\n",
    "    resource_list = map(lambda r: [r['time'], r['memory'].used, r['cpu_total']], resource_usage)\n",
    "    resource_df = pd.DataFrame(resource_list, columns = ['time', 'memory_used', 'cpu_used'])\n",
    "    resource_df.to_csv(respath + f'/resource-usage-{run}.csv')\n",
    "\n",
    "    peak_memory = max(map(lambda r: r['memory'].used, resource_usage)) / (1000 * 1000 * 1000) # GB\n",
    "\n",
    "    if execution is not None:\n",
    "            with open(respath + f'/execution-{run}.json', 'w') as f:\n",
    "                f.write(json.dumps(execution, indent=2))\n",
    "            with open(respath + f'/jobs-{run}.json', 'w') as f:\n",
    "                f.write(json.dumps(jobs, indent=2))\n",
    "            with open(respath + f'/stages-{run}.json', 'w') as f:\n",
    "                f.write(json.dumps(job_stages, indent=2))\n",
    "    return (diff_time, peak_memory)\n",
    "\n",
    "def benchmark(spark, dbname, query_file, mode, run):\n",
    "    #spark.sql(\"SET spark.sql.yannakakis.enabled = false\").show()\n",
    "    # run the query once to warm up Spark (load the relation in memory)\n",
    "    #df0 = run_query(query)\n",
    "    #df0.show()\n",
    "    \n",
    "    query_name = os.path.basename(query_file)\n",
    "\n",
    "    respath = f'benchmark-results-{dbname}/' + query_name + \"/\" + mode\n",
    "    pathlib.Path(respath).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if mode == \"opt\":\n",
    "        spark.sql(\"SET spark.sql.yannakakis.enabled = true\").show()\n",
    "    elif mode == \"ref\":\n",
    "        spark.sql(\"SET spark.sql.yannakakis.enabled = false\").show()\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        (runtime, peak_memory) = benchmark_query(spark, query_file, respath, run)\n",
    "        return [query_name, runtime, peak_memory, mode, run]\n",
    "    except Py4JError as e:\n",
    "        print('timeout or error: ' + str(e))\n",
    "        return [query_name, None, None, mode, run]\n",
    "\n",
    "def benchmark_all(dbname, mode, runs, queries, group_in_leaves=False):\n",
    "    spark = create_spark()\n",
    "    import_db(spark, dbname)\n",
    "\n",
    "    if group_in_leaves:\n",
    "        spark.sql(\"SET spark.sql.yannakakis.countGroupInLeaves = true\").show()\n",
    "    else:\n",
    "        spark.sql(\"SET spark.sql.yannakakis.countGroupInLeaves = false\").show()\n",
    "\n",
    "    results_df = df = pd.DataFrame([], columns = ['query', 'runtime', 'peak_memory', 'mode', 'run'])\n",
    "    results_file = f'benchmark-results-{dbname}/results-{mode}.csv'\n",
    "    if (os.path.exists(results_file)):\n",
    "        results_df = pd.read_csv(results_file, index_col=0)\n",
    "\n",
    "    for run in runs:\n",
    "        for q in queries:\n",
    "            results = [benchmark(spark, dbname, q, mode, run)]\n",
    "            new_df = pd.DataFrame(results, columns = ['query', 'runtime', 'peak_memory', 'mode', 'run'])\n",
    "            results_df = pd.concat([results_df, new_df], ignore_index=True)\n",
    "            results_df.to_csv(f'benchmark-results-{dbname}/results-{mode}.csv')\n",
    "            print(results_df)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398c3837-5334-46f7-9f4e-b01966e8749a",
   "metadata": {},
   "source": [
    "## SNAP Benchmark\n",
    "\n",
    "### Optimized execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be8c1d5-2fe0-4853-81ae-56f40f8a8097",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### benchmark configuration\n",
    "group_in_leaves = False\n",
    "dbname = 'snap'\n",
    "mode = 'opt'\n",
    "runs = ['1', '2', '3', '4', '5', '6']\n",
    "####\n",
    "\n",
    "tables = ['patents', 'wiki', 'google', 'dblp']\n",
    "\n",
    "for tablename in tables:\n",
    "    queries = sorted(glob.glob(f'snap-queries/all/{tablename}-*'))\n",
    "    print('running queries: ' + str(queries))\n",
    "    benchmark_all(dbname, mode, runs, queries)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0e5d6e-463b-4fe5-afec-a01bef351ce4",
   "metadata": {},
   "source": [
    "### Ref execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4c01dd-4cd3-4cb7-8e44-58f9deb2327a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### benchmark configuration\n",
    "group_in_leaves = False\n",
    "dbname = 'snap'\n",
    "mode = 'ref'\n",
    "runs = ['1', '2', '3', '4', '5', '6']\n",
    "####\n",
    "\n",
    "queries = ['snap-queries/all/patents-path02.sql',\n",
    "          'snap-queries/all/patents-path03.sql',\n",
    "          'snap-queries/all/patents-path04.sql',\n",
    "          'snap-queries/all/patents-path05.sql',\n",
    "          'snap-queries/all/patents-tree01.sql',\n",
    "          'snap-queries/all/wiki-path02.sql',\n",
    "           'snap-queries/all/google-path02.sql',\n",
    "           'snap-queries/all/google-path03.sql',\n",
    "           'snap-queries/all/google-path04.sql',\n",
    "           'snap-queries/all/dblp-path02.sql',\n",
    "           'snap-queries/all/dblp-path03.sql',\n",
    "           'snap-queries/all/dblp-path04.sql',\n",
    "           'snap-queries/all/dblp-path05.sql',\n",
    "           'snap-queries/all/dblp-tree01.sql',\n",
    "           'snap-queries/all/dblp-tree02.sql'\n",
    "          ]\n",
    "\n",
    "\n",
    "print('running queries: ' + str(queries))\n",
    "\n",
    "benchmark_all(dbname, mode, runs, queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a937d945-3c03-494f-8a5f-8f1f69bf5a22",
   "metadata": {},
   "source": [
    "## LSQB Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7beb471-30e3-41c8-a6ba-4aac25bc4102",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### benchmark configuration\n",
    "dbname = 'lsqb'\n",
    "#mode = 'opt'\n",
    "runs = ['1', '2', '3', '4', '5', '6']\n",
    "####\n",
    "\n",
    "queries = ['lsqb/sql/q1.sql', 'lsqb/sql/q4.sql']\n",
    "queries_hints = ['lsqb/sql/q1-hint.sql', 'lsqb/sql/q4-hint.sql']\n",
    "\n",
    "print('running queries: ' + str(queries))\n",
    "benchmark_all(dbname, 'opt', runs, queries, group_in_leaves=False)\n",
    "benchmark_all(dbname, 'opt', runs, queries_hints, group_in_leaves=False)\n",
    "benchmark_all(dbname, 'ref', runs, queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09492e05",
   "metadata": {},
   "source": [
    "## TPC-H Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67544c5b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### benchmark configuration\n",
    "group_in_leaves = True\n",
    "dbname = 'tpch'\n",
    "runs = ['1', '2', '3', '4', '5', '6']\n",
    "####\n",
    "\n",
    "queries = ['tpch-kit/dbgen/queries/postgres/2.sql',\n",
    "           'tpch-kit/dbgen/queries/postgres/11.sql', \n",
    "           'tpch-kit/dbgen/queries/postgres/11-hint.sql']\n",
    "queries += ['tpch-queries/median-1.sql', 'tpch-queries/median-1-hint.sql']\n",
    "#queries = ['tpch-queries/2-subq.sql'] #, 'tpch-queries/2-subq-hint.sql']\n",
    "\n",
    "print('running queries: ' + str(queries))\n",
    "benchmark_all(dbname, 'ref', runs, queries)\n",
    "benchmark_all(dbname, 'opt', runs, queries, group_in_leaves = group_in_leaves)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380628a7-8b9f-40b6-b2ba-c4089897e84f",
   "metadata": {},
   "source": [
    "## JOB (IMDB) Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251fb301-811b-49ab-ada6-8f2b5d566031",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### benchmark configuration\n",
    "group_in_leaves = False\n",
    "dbname = 'imdb'\n",
    "runs = ['1', '2', '3', '4', '5', '6']\n",
    "####\n",
    "\n",
    "queries = ['job/2a.sql', 'job/2b.sql', 'job/2c.sql', 'job/2d.sql',\n",
    "           'job/3a.sql', 'job/3b.sql', 'job/3c.sql',\n",
    "           'job/5a.sql', 'job/5b.sql', 'job/5c.sql',\n",
    "           'job/17a.sql', 'job/17b.sql', 'job/17c.sql', 'job/17d.sql', 'job/17e.sql', 'job/17f.sql',\n",
    "           'job/20a.sql', 'job/20b.sql', 'job/20c.sql',\n",
    "          ]\n",
    "\n",
    "print('running queries: ' + str(queries))\n",
    "benchmark_all(dbname, 'opt', runs, queries)\n",
    "benchmark_all(dbname, 'ref', runs, queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c1c9eb-9e98-43cd-9cdb-e6910759b78f",
   "metadata": {},
   "source": [
    "## STATS Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18dc713f-1a8a-473d-ae10-b126963873f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### benchmark configuration\n",
    "dbname = 'stats'\n",
    "#mode = 'opt'\n",
    "runs = ['1', '2', '3', '4', '5', '6']\n",
    "#runs = ['04']\n",
    "#runs = ['01']\n",
    "####\n",
    "\n",
    "queries = sorted(glob.glob('stats-queries/*.sql'))\n",
    "queries_hint = sorted(glob.glob('stats-queries/hints/*.sql'))\n",
    "\n",
    "print('running queries: ' + str(queries))\n",
    "benchmark_all(dbname, 'opt', runs, queries)\n",
    "benchmark_all(dbname, 'opt', runs, queries_hint, group_in_leaves=True)\n",
    "benchmark_all(dbname, 'ref', runs, queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08624383-3a60-402d-9a3d-3d5bd44b9a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/12 15:54:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tagclass\n",
      "forum\n",
      "comment\n",
      "post\n",
      "person\n",
      "comment_hastag_tag\n",
      "company\n",
      "university\n",
      "continent\n",
      "country\n",
      "city\n",
      "tag\n",
      "post_hastag_tag\n",
      "forum_hasmember_person\n",
      "forum_hastag_tag\n",
      "person_hasinterest_tag\n",
      "person_likes_comment\n",
      "person_likes_post\n",
      "person_studyat_university\n",
      "person_workat_company\n",
      "person_knows_person\n",
      "message\n",
      "comment_replyof_message\n",
      "message_hascreator_person\n",
      "message_hastag_tag\n",
      "message_islocatedin_country\n",
      "person_likes_message\n"
     ]
    }
   ],
   "source": [
    "spark = create_spark()\n",
    "#import_db(spark, 'stats')\n",
    "import_db(spark, 'lsqb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea49cfe-1fa5-41d2-b621-0d9a8b5a3958",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                 key|value|\n",
      "+--------------------+-----+\n",
      "|spark.sql.yannaka...|false|\n",
      "+--------------------+-----+\n",
      "\n",
      "running query: \n",
      "SELECT count(*)\n",
      "\n",
      "FROM Country\n",
      "\n",
      "JOIN City\n",
      "\n",
      "  ON City.isPartOf_CountryId = Country.CountryId\n",
      "\n",
      "JOIN Person\n",
      "\n",
      "  ON Person.isLocatedIn_CityId = City.CityId\n",
      "\n",
      "JOIN Forum_hasMember_Person\n",
      "\n",
      "  ON Forum_hasMember_Person.PersonId = Person.PersonId\n",
      "\n",
      "JOIN Forum\n",
      "\n",
      "  ON Forum.ForumId = Forum_hasMember_Person.ForumId\n",
      "\n",
      "JOIN Post\n",
      "\n",
      "  ON Post.Forum_containerOfId = Forum.ForumId\n",
      "\n",
      "JOIN Comment\n",
      "\n",
      "  ON Comment.replyOf_PostId = Post.PostId\n",
      "\n",
      "JOIN Comment_hasTag_Tag\n",
      "\n",
      "  ON Comment_hasTag_Tag.CommentId = Comment.CommentId\n",
      "\n",
      "JOIN Tag\n",
      "\n",
      "  ON Tag.TagId = Comment_hasTag_Tag.TagId\n",
      "\n",
      "JOIN TagClass\n",
      "\n",
      "  ON Tag.hasType_TagClassId = TagClass.TagClassId;\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/12 15:57:30 INFO CodeGenerator: Code generated in 3.397606 ms\n",
      "23/12/12 15:57:30 INFO DAGScheduler: Registering RDD 218 (showString at NativeMethodAccessorImpl.java:0) as input to shuffle 41\n",
      "23/12/12 15:57:30 INFO DAGScheduler: Got map stage job 55 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "23/12/12 15:57:30 INFO DAGScheduler: Final stage: ShuffleMapStage 159 (showString at NativeMethodAccessorImpl.java:0)\n",
      "23/12/12 15:57:30 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/12/12 15:57:30 INFO DAGScheduler: Missing parents: List()\n",
      "23/12/12 15:57:30 INFO DAGScheduler: Submitting ShuffleMapStage 159 (MapPartitionsRDD[218] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/12/12 15:57:31 INFO MemoryStore: Block broadcast_66 stored as values in memory (estimated size 13.9 KiB, free 29.8 GiB)\n",
      "23/12/12 15:57:31 INFO MemoryStore: Block broadcast_66_piece0 stored as bytes in memory (estimated size 7.3 KiB, free 29.8 GiB)\n",
      "23/12/12 15:57:31 INFO BlockManagerInfo: Added broadcast_66_piece0 in memory on 53aa7a5d52e0:36993 (size: 7.3 KiB, free: 29.8 GiB)\n",
      "23/12/12 15:57:31 INFO SparkContext: Created broadcast 66 from broadcast at DAGScheduler.scala:1580\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 159 (MapPartitionsRDD[218] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "23/12/12 15:57:31 INFO TaskSchedulerImpl: Adding task set 159.0 with 1 tasks resource profile 0\n",
      "23/12/12 15:57:31 INFO CodeGenerator: Code generated in 2.8452 ms\n",
      "23/12/12 15:57:31 INFO TaskSetManager: Starting task 0.0 in stage 159.0 (TID 174) (53aa7a5d52e0, executor driver, partition 0, PROCESS_LOCAL, 7630 bytes) \n",
      "23/12/12 15:57:31 INFO Executor: Running task 0.0 in stage 159.0 (TID 174)\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Registering RDD 220 (showString at NativeMethodAccessorImpl.java:0) as input to shuffle 42\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Got map stage job 56 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Final stage: ShuffleMapStage 160 (showString at NativeMethodAccessorImpl.java:0)\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Missing parents: List()\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Submitting ShuffleMapStage 160 (MapPartitionsRDD[220] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/12/12 15:57:31 INFO MemoryStore: Block broadcast_67 stored as values in memory (estimated size 14.4 KiB, free 29.8 GiB)\n",
      "23/12/12 15:57:31 INFO MemoryStore: Block broadcast_67_piece0 stored as bytes in memory (estimated size 7.5 KiB, free 29.8 GiB)\n",
      "23/12/12 15:57:31 INFO BlockManagerInfo: Added broadcast_67_piece0 in memory on 53aa7a5d52e0:36993 (size: 7.5 KiB, free: 29.8 GiB)\n",
      "23/12/12 15:57:31 INFO SparkContext: Created broadcast 67 from broadcast at DAGScheduler.scala:1580\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 160 (MapPartitionsRDD[220] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "23/12/12 15:57:31 INFO TaskSchedulerImpl: Adding task set 160.0 with 1 tasks resource profile 0\n",
      "23/12/12 15:57:31 INFO CodeGenerator: Code generated in 2.651398 ms\n",
      "23/12/12 15:57:31 INFO TaskSetManager: Starting task 0.0 in stage 160.0 (TID 175) (53aa7a5d52e0, executor driver, partition 0, PROCESS_LOCAL, 7630 bytes) \n",
      "23/12/12 15:57:31 INFO Executor: Running task 0.0 in stage 160.0 (TID 175)\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Registering RDD 222 (showString at NativeMethodAccessorImpl.java:0) as input to shuffle 43\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Got map stage job 57 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Final stage: ShuffleMapStage 161 (showString at NativeMethodAccessorImpl.java:0)\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Missing parents: List()\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Submitting ShuffleMapStage 161 (MapPartitionsRDD[222] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/12/12 15:57:31 INFO MemoryStore: Block broadcast_68 stored as values in memory (estimated size 14.4 KiB, free 29.8 GiB)\n",
      "23/12/12 15:57:31 INFO MemoryStore: Block broadcast_68_piece0 stored as bytes in memory (estimated size 7.5 KiB, free 29.8 GiB)\n",
      "23/12/12 15:57:31 INFO BlockManagerInfo: Added broadcast_68_piece0 in memory on 53aa7a5d52e0:36993 (size: 7.5 KiB, free: 29.8 GiB)\n",
      "23/12/12 15:57:31 INFO SparkContext: Created broadcast 68 from broadcast at DAGScheduler.scala:1580\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 161 (MapPartitionsRDD[222] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "23/12/12 15:57:31 INFO TaskSchedulerImpl: Adding task set 161.0 with 1 tasks resource profile 0\n",
      "23/12/12 15:57:31 INFO TaskSetManager: Starting task 0.0 in stage 161.0 (TID 176) (53aa7a5d52e0, executor driver, partition 0, PROCESS_LOCAL, 7630 bytes) \n",
      "23/12/12 15:57:31 INFO Executor: Running task 0.0 in stage 161.0 (TID 176)\n",
      "23/12/12 15:57:31 INFO CodeGenerator: Code generated in 3.044524 ms\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Registering RDD 224 (showString at NativeMethodAccessorImpl.java:0) as input to shuffle 44\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Got map stage job 58 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Final stage: ShuffleMapStage 162 (showString at NativeMethodAccessorImpl.java:0)\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Missing parents: List()\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Submitting ShuffleMapStage 162 (MapPartitionsRDD[224] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/12/12 15:57:31 INFO MemoryStore: Block broadcast_69 stored as values in memory (estimated size 14.4 KiB, free 29.8 GiB)\n",
      "23/12/12 15:57:31 INFO MemoryStore: Block broadcast_69_piece0 stored as bytes in memory (estimated size 7.5 KiB, free 29.8 GiB)\n",
      "23/12/12 15:57:31 INFO BlockManagerInfo: Added broadcast_69_piece0 in memory on 53aa7a5d52e0:36993 (size: 7.5 KiB, free: 29.8 GiB)\n",
      "23/12/12 15:57:31 INFO SparkContext: Created broadcast 69 from broadcast at DAGScheduler.scala:1580\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 162 (MapPartitionsRDD[224] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "23/12/12 15:57:31 INFO TaskSchedulerImpl: Adding task set 162.0 with 1 tasks resource profile 0\n",
      "23/12/12 15:57:31 INFO TaskSetManager: Starting task 0.0 in stage 162.0 (TID 177) (53aa7a5d52e0, executor driver, partition 0, PROCESS_LOCAL, 7630 bytes) \n",
      "23/12/12 15:57:31 INFO Executor: Running task 0.0 in stage 162.0 (TID 177)\n",
      "23/12/12 15:57:31 INFO CodeGenerator: Code generated in 2.795187 ms\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Registering RDD 226 (showString at NativeMethodAccessorImpl.java:0) as input to shuffle 45\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Got map stage job 59 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Final stage: ShuffleMapStage 163 (showString at NativeMethodAccessorImpl.java:0)\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Missing parents: List()\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Submitting ShuffleMapStage 163 (MapPartitionsRDD[226] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/12/12 15:57:31 INFO MemoryStore: Block broadcast_70 stored as values in memory (estimated size 13.9 KiB, free 29.8 GiB)\n",
      "23/12/12 15:57:31 INFO JDBCRDD: closed connection\n",
      "23/12/12 15:57:31 INFO MemoryStore: Block broadcast_70_piece0 stored as bytes in memory (estimated size 7.3 KiB, free 29.8 GiB)\n",
      "23/12/12 15:57:31 INFO BlockManagerInfo: Added broadcast_70_piece0 in memory on 53aa7a5d52e0:36993 (size: 7.3 KiB, free: 29.8 GiB)\n",
      "23/12/12 15:57:31 INFO SparkContext: Created broadcast 70 from broadcast at DAGScheduler.scala:1580\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 163 (MapPartitionsRDD[226] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "23/12/12 15:57:31 INFO TaskSchedulerImpl: Adding task set 163.0 with 1 tasks resource profile 0\n",
      "23/12/12 15:57:31 INFO TaskSetManager: Starting task 0.0 in stage 163.0 (TID 178) (53aa7a5d52e0, executor driver, partition 0, PROCESS_LOCAL, 7630 bytes) \n",
      "23/12/12 15:57:31 INFO Executor: Running task 0.0 in stage 163.0 (TID 178)\n",
      "23/12/12 15:57:31 INFO CodeGenerator: Code generated in 2.917727 ms\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Registering RDD 228 (showString at NativeMethodAccessorImpl.java:0) as input to shuffle 46\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Got map stage job 60 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Final stage: ShuffleMapStage 164 (showString at NativeMethodAccessorImpl.java:0)\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Missing parents: List()\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Submitting ShuffleMapStage 164 (MapPartitionsRDD[228] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/12/12 15:57:31 INFO MemoryStore: Block broadcast_71 stored as values in memory (estimated size 14.7 KiB, free 29.8 GiB)\n",
      "23/12/12 15:57:31 INFO MemoryStore: Block broadcast_71_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 29.8 GiB)\n",
      "23/12/12 15:57:31 INFO JDBCRDD: closed connection\n",
      "23/12/12 15:57:31 INFO BlockManagerInfo: Added broadcast_71_piece0 in memory on 53aa7a5d52e0:36993 (size: 7.7 KiB, free: 29.8 GiB)\n",
      "23/12/12 15:57:31 INFO SparkContext: Created broadcast 71 from broadcast at DAGScheduler.scala:1580\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 164 (MapPartitionsRDD[228] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "23/12/12 15:57:31 INFO TaskSchedulerImpl: Adding task set 164.0 with 1 tasks resource profile 0\n",
      "23/12/12 15:57:31 INFO TaskSetManager: Starting task 0.0 in stage 164.0 (TID 179) (53aa7a5d52e0, executor driver, partition 0, PROCESS_LOCAL, 7630 bytes) \n",
      "23/12/12 15:57:31 INFO Executor: Running task 0.0 in stage 164.0 (TID 179)\n",
      "23/12/12 15:57:31 INFO CodeGenerator: Code generated in 3.724248 ms\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Registering RDD 230 (showString at NativeMethodAccessorImpl.java:0) as input to shuffle 47\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Got map stage job 61 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Final stage: ShuffleMapStage 165 (showString at NativeMethodAccessorImpl.java:0)\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Missing parents: List()\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Submitting ShuffleMapStage 165 (MapPartitionsRDD[230] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/12/12 15:57:31 INFO MemoryStore: Block broadcast_72 stored as values in memory (estimated size 14.4 KiB, free 29.8 GiB)\n",
      "23/12/12 15:57:31 INFO Executor: Finished task 0.0 in stage 159.0 (TID 174). 1946 bytes result sent to driver\n",
      "23/12/12 15:57:31 INFO TaskSetManager: Finished task 0.0 in stage 159.0 (TID 174) in 30 ms on 53aa7a5d52e0 (executor driver) (1/1)\n",
      "23/12/12 15:57:31 INFO MemoryStore: Block broadcast_72_piece0 stored as bytes in memory (estimated size 7.5 KiB, free 29.8 GiB)\n",
      "23/12/12 15:57:31 INFO TaskSchedulerImpl: Removed TaskSet 159.0, whose tasks have all completed, from pool \n",
      "23/12/12 15:57:31 INFO BlockManagerInfo: Added broadcast_72_piece0 in memory on 53aa7a5d52e0:36993 (size: 7.5 KiB, free: 29.8 GiB)\n",
      "23/12/12 15:57:31 INFO SparkContext: Created broadcast 72 from broadcast at DAGScheduler.scala:1580\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 165 (MapPartitionsRDD[230] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "23/12/12 15:57:31 INFO TaskSchedulerImpl: Adding task set 165.0 with 1 tasks resource profile 0\n",
      "23/12/12 15:57:31 INFO DAGScheduler: ShuffleMapStage 159 (showString at NativeMethodAccessorImpl.java:0) finished in 0.034 s\n",
      "23/12/12 15:57:31 INFO DAGScheduler: looking for newly runnable stages\n",
      "23/12/12 15:57:31 INFO CodeGenerator: Code generated in 2.958743 ms\n",
      "23/12/12 15:57:31 INFO DAGScheduler: running: Set(ShuffleMapStage 161, ShuffleMapStage 165, ShuffleMapStage 162, ShuffleMapStage 163, ShuffleMapStage 160, ShuffleMapStage 164)\n",
      "23/12/12 15:57:31 INFO DAGScheduler: waiting: Set()\n",
      "23/12/12 15:57:31 INFO DAGScheduler: failed: Set()\n",
      "23/12/12 15:57:31 INFO TaskSetManager: Starting task 0.0 in stage 165.0 (TID 180) (53aa7a5d52e0, executor driver, partition 0, PROCESS_LOCAL, 7630 bytes) \n",
      "23/12/12 15:57:31 INFO Executor: Running task 0.0 in stage 165.0 (TID 180)\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Registering RDD 232 (showString at NativeMethodAccessorImpl.java:0) as input to shuffle 48\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Got map stage job 62 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Final stage: ShuffleMapStage 166 (showString at NativeMethodAccessorImpl.java:0)\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Missing parents: List()\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Submitting ShuffleMapStage 166 (MapPartitionsRDD[232] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/12/12 15:57:31 INFO Executor: Finished task 0.0 in stage 160.0 (TID 175). 1946 bytes result sent to driver\n",
      "23/12/12 15:57:31 INFO MemoryStore: Block broadcast_73 stored as values in memory (estimated size 14.4 KiB, free 29.8 GiB)\n",
      "23/12/12 15:57:31 INFO TaskSetManager: Finished task 0.0 in stage 160.0 (TID 175) in 30 ms on 53aa7a5d52e0 (executor driver) (1/1)\n",
      "23/12/12 15:57:31 INFO TaskSchedulerImpl: Removed TaskSet 160.0, whose tasks have all completed, from pool \n",
      "23/12/12 15:57:31 INFO MemoryStore: Block broadcast_73_piece0 stored as bytes in memory (estimated size 7.5 KiB, free 29.8 GiB)\n",
      "23/12/12 15:57:31 INFO BlockManagerInfo: Added broadcast_73_piece0 in memory on 53aa7a5d52e0:36993 (size: 7.5 KiB, free: 29.8 GiB)\n",
      "23/12/12 15:57:31 INFO SparkContext: Created broadcast 73 from broadcast at DAGScheduler.scala:1580\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 166 (MapPartitionsRDD[232] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "23/12/12 15:57:31 INFO TaskSchedulerImpl: Adding task set 166.0 with 1 tasks resource profile 0\n",
      "23/12/12 15:57:31 INFO DAGScheduler: ShuffleMapStage 160 (showString at NativeMethodAccessorImpl.java:0) finished in 0.034 s\n",
      "23/12/12 15:57:31 INFO TaskSetManager: Starting task 0.0 in stage 166.0 (TID 181) (53aa7a5d52e0, executor driver, partition 0, PROCESS_LOCAL, 7630 bytes) \n",
      "23/12/12 15:57:31 INFO DAGScheduler: looking for newly runnable stages\n",
      "23/12/12 15:57:31 INFO DAGScheduler: running: Set(ShuffleMapStage 161, ShuffleMapStage 165, ShuffleMapStage 166, ShuffleMapStage 162, ShuffleMapStage 163, ShuffleMapStage 164)\n",
      "23/12/12 15:57:31 INFO DAGScheduler: waiting: Set()\n",
      "23/12/12 15:57:31 INFO DAGScheduler: failed: Set()\n",
      "23/12/12 15:57:31 INFO Executor: Running task 0.0 in stage 166.0 (TID 181)\n",
      "23/12/12 15:57:31 INFO CodeGenerator: Code generated in 3.278041 ms\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Registering RDD 234 (showString at NativeMethodAccessorImpl.java:0) as input to shuffle 49\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Got map stage job 63 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Final stage: ShuffleMapStage 167 (showString at NativeMethodAccessorImpl.java:0)\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Missing parents: List()\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Submitting ShuffleMapStage 167 (MapPartitionsRDD[234] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/12/12 15:57:31 INFO MemoryStore: Block broadcast_74 stored as values in memory (estimated size 14.4 KiB, free 29.8 GiB)\n",
      "23/12/12 15:57:31 INFO CodeGenerator: Code generated in 8.856834 ms\n",
      "23/12/12 15:57:31 INFO MemoryStore: Block broadcast_74_piece0 stored as bytes in memory (estimated size 7.5 KiB, free 29.8 GiB)\n",
      "23/12/12 15:57:31 INFO BlockManagerInfo: Added broadcast_74_piece0 in memory on 53aa7a5d52e0:36993 (size: 7.5 KiB, free: 29.8 GiB)\n",
      "23/12/12 15:57:31 INFO SparkContext: Created broadcast 74 from broadcast at DAGScheduler.scala:1580\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 167 (MapPartitionsRDD[234] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "23/12/12 15:57:31 INFO TaskSchedulerImpl: Adding task set 167.0 with 1 tasks resource profile 0\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Registering RDD 236 (showString at NativeMethodAccessorImpl.java:0) as input to shuffle 50\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Got map stage job 64 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Final stage: ShuffleMapStage 168 (showString at NativeMethodAccessorImpl.java:0)\n",
      "23/12/12 15:57:31 INFO TaskSetManager: Starting task 0.0 in stage 167.0 (TID 182) (53aa7a5d52e0, executor driver, partition 0, PROCESS_LOCAL, 7630 bytes) \n",
      "23/12/12 15:57:31 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Missing parents: List()\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Submitting ShuffleMapStage 168 (MapPartitionsRDD[236] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/12/12 15:57:31 INFO Executor: Running task 0.0 in stage 167.0 (TID 182)\n",
      "23/12/12 15:57:31 INFO MemoryStore: Block broadcast_75 stored as values in memory (estimated size 13.9 KiB, free 29.8 GiB)\n",
      "23/12/12 15:57:31 INFO MemoryStore: Block broadcast_75_piece0 stored as bytes in memory (estimated size 7.3 KiB, free 29.8 GiB)\n",
      "23/12/12 15:57:31 INFO BlockManagerInfo: Added broadcast_75_piece0 in memory on 53aa7a5d52e0:36993 (size: 7.3 KiB, free: 29.8 GiB)\n",
      "23/12/12 15:57:31 INFO SparkContext: Created broadcast 75 from broadcast at DAGScheduler.scala:1580\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 168 (MapPartitionsRDD[236] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "23/12/12 15:57:31 INFO TaskSchedulerImpl: Adding task set 168.0 with 1 tasks resource profile 0\n",
      "23/12/12 15:57:31 INFO TaskSetManager: Starting task 0.0 in stage 168.0 (TID 183) (53aa7a5d52e0, executor driver, partition 0, PROCESS_LOCAL, 7630 bytes) \n",
      "23/12/12 15:57:31 INFO Executor: Running task 0.0 in stage 168.0 (TID 183)\n",
      "23/12/12 15:57:31 INFO ShufflePartitionsUtil: For shuffle(41), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "23/12/12 15:57:31 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "23/12/12 15:57:31 INFO JDBCRDD: closed connection\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Got job 65 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Final stage: ResultStage 170 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 169)\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Missing parents: List()\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Submitting ResultStage 170 (MapPartitionsRDD[238] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
      "23/12/12 15:57:31 INFO MemoryStore: Block broadcast_76 stored as values in memory (estimated size 8.3 KiB, free 29.8 GiB)\n",
      "23/12/12 15:57:31 INFO MemoryStore: Block broadcast_76_piece0 stored as bytes in memory (estimated size 4.2 KiB, free 29.8 GiB)\n",
      "23/12/12 15:57:31 INFO BlockManagerInfo: Added broadcast_76_piece0 in memory on 53aa7a5d52e0:36993 (size: 4.2 KiB, free: 29.8 GiB)\n",
      "23/12/12 15:57:31 INFO SparkContext: Created broadcast 76 from broadcast at DAGScheduler.scala:1580\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 170 (MapPartitionsRDD[238] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
      "23/12/12 15:57:31 INFO TaskSchedulerImpl: Adding task set 170.0 with 1 tasks resource profile 0\n",
      "23/12/12 15:57:31 INFO JDBCRDD: closed connection\n",
      "23/12/12 15:57:31 INFO Executor: Finished task 0.0 in stage 168.0 (TID 183). 1946 bytes result sent to driver\n",
      "23/12/12 15:57:31 INFO TaskSetManager: Starting task 0.0 in stage 170.0 (TID 184) (53aa7a5d52e0, executor driver, partition 0, NODE_LOCAL, 7814 bytes) \n",
      "23/12/12 15:57:31 INFO TaskSetManager: Finished task 0.0 in stage 168.0 (TID 183) in 23 ms on 53aa7a5d52e0 (executor driver) (1/1)\n",
      "23/12/12 15:57:31 INFO Executor: Running task 0.0 in stage 170.0 (TID 184)\n",
      "23/12/12 15:57:31 INFO TaskSchedulerImpl: Removed TaskSet 168.0, whose tasks have all completed, from pool \n",
      "23/12/12 15:57:31 INFO DAGScheduler: ShuffleMapStage 168 (showString at NativeMethodAccessorImpl.java:0) finished in 0.027 s\n",
      "23/12/12 15:57:31 INFO DAGScheduler: looking for newly runnable stages\n",
      "23/12/12 15:57:31 INFO DAGScheduler: running: Set(ShuffleMapStage 161, ShuffleMapStage 165, ShuffleMapStage 166, ResultStage 170, ShuffleMapStage 162, ShuffleMapStage 163, ShuffleMapStage 167, ShuffleMapStage 164)\n",
      "23/12/12 15:57:31 INFO DAGScheduler: waiting: Set()\n",
      "23/12/12 15:57:31 INFO DAGScheduler: failed: Set()\n",
      "23/12/12 15:57:31 INFO ShuffleBlockFetcherIterator: Getting 1 (4.9 KiB) non-empty blocks including 1 (4.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "23/12/12 15:57:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "23/12/12 15:57:31 INFO Executor: Finished task 0.0 in stage 170.0 (TID 184). 4365 bytes result sent to driver\n",
      "23/12/12 15:57:31 INFO TaskSetManager: Finished task 0.0 in stage 170.0 (TID 184) in 5 ms on 53aa7a5d52e0 (executor driver) (1/1)\n",
      "23/12/12 15:57:31 INFO TaskSchedulerImpl: Removed TaskSet 170.0, whose tasks have all completed, from pool \n",
      "23/12/12 15:57:31 INFO DAGScheduler: ResultStage 170 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.011 s\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Job 65 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/12/12 15:57:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 170: Stage finished\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Job 65 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.013434 s\n",
      "23/12/12 15:57:31 INFO MemoryStore: Block broadcast_77 stored as values in memory (estimated size 1024.9 KiB, free 29.8 GiB)\n",
      "23/12/12 15:57:31 INFO MemoryStore: Block broadcast_77_piece0 stored as bytes in memory (estimated size 1503.0 B, free 29.8 GiB)\n",
      "23/12/12 15:57:31 INFO JDBCRDD: closed connection\n",
      "23/12/12 15:57:31 INFO BlockManagerInfo: Added broadcast_77_piece0 in memory on 53aa7a5d52e0:36993 (size: 1503.0 B, free: 29.8 GiB)\n",
      "23/12/12 15:57:31 INFO SparkContext: Created broadcast 77 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "23/12/12 15:57:31 INFO ShufflePartitionsUtil: For shuffle(50), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "23/12/12 15:57:31 INFO Executor: Finished task 0.0 in stage 161.0 (TID 176). 1946 bytes result sent to driver\n",
      "23/12/12 15:57:31 INFO TaskSetManager: Finished task 0.0 in stage 161.0 (TID 176) in 85 ms on 53aa7a5d52e0 (executor driver) (1/1)\n",
      "23/12/12 15:57:31 INFO TaskSchedulerImpl: Removed TaskSet 161.0, whose tasks have all completed, from pool \n",
      "23/12/12 15:57:31 INFO DAGScheduler: ShuffleMapStage 161 (showString at NativeMethodAccessorImpl.java:0) finished in 0.088 s\n",
      "23/12/12 15:57:31 INFO DAGScheduler: looking for newly runnable stages\n",
      "23/12/12 15:57:31 INFO DAGScheduler: running: Set(ShuffleMapStage 165, ShuffleMapStage 166, ShuffleMapStage 162, ShuffleMapStage 163, ShuffleMapStage 167, ShuffleMapStage 164)\n",
      "23/12/12 15:57:31 INFO DAGScheduler: waiting: Set()\n",
      "23/12/12 15:57:31 INFO DAGScheduler: failed: Set()\n",
      "23/12/12 15:57:31 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Got job 66 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Final stage: ResultStage 172 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 171)\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Missing parents: List()\n",
      "23/12/12 15:57:31 INFO ShufflePartitionsUtil: For shuffle(42), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Submitting ResultStage 172 (MapPartitionsRDD[240] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
      "23/12/12 15:57:31 INFO MemoryStore: Block broadcast_78 stored as values in memory (estimated size 8.3 KiB, free 29.8 GiB)\n",
      "23/12/12 15:57:31 INFO MemoryStore: Block broadcast_78_piece0 stored as bytes in memory (estimated size 4.2 KiB, free 29.8 GiB)\n",
      "23/12/12 15:57:31 INFO BlockManagerInfo: Added broadcast_78_piece0 in memory on 53aa7a5d52e0:36993 (size: 4.2 KiB, free: 29.8 GiB)\n",
      "23/12/12 15:57:31 INFO SparkContext: Created broadcast 78 from broadcast at DAGScheduler.scala:1580\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 172 (MapPartitionsRDD[240] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
      "23/12/12 15:57:31 INFO TaskSchedulerImpl: Adding task set 172.0 with 1 tasks resource profile 0\n",
      "23/12/12 15:57:31 INFO TaskSetManager: Starting task 0.0 in stage 172.0 (TID 185) (53aa7a5d52e0, executor driver, partition 0, NODE_LOCAL, 7814 bytes) \n",
      "23/12/12 15:57:31 INFO Executor: Running task 0.0 in stage 172.0 (TID 185)\n",
      "23/12/12 15:57:31 INFO Executor: Finished task 0.0 in stage 167.0 (TID 182). 1946 bytes result sent to driver\n",
      "23/12/12 15:57:31 INFO TaskSetManager: Finished task 0.0 in stage 167.0 (TID 182) in 52 ms on 53aa7a5d52e0 (executor driver) (1/1)\n",
      "23/12/12 15:57:31 INFO TaskSchedulerImpl: Removed TaskSet 167.0, whose tasks have all completed, from pool \n",
      "23/12/12 15:57:31 INFO DAGScheduler: ShuffleMapStage 167 (showString at NativeMethodAccessorImpl.java:0) finished in 0.056 s\n",
      "23/12/12 15:57:31 INFO DAGScheduler: looking for newly runnable stages\n",
      "23/12/12 15:57:31 INFO DAGScheduler: running: Set(ShuffleMapStage 165, ResultStage 172, ShuffleMapStage 166, ShuffleMapStage 162, ShuffleMapStage 163, ShuffleMapStage 164)\n",
      "23/12/12 15:57:31 INFO DAGScheduler: waiting: Set()\n",
      "23/12/12 15:57:31 INFO DAGScheduler: failed: Set()\n",
      "23/12/12 15:57:31 INFO ShuffleBlockFetcherIterator: Getting 1 (3.5 KiB) non-empty blocks including 1 (3.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "23/12/12 15:57:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "23/12/12 15:57:31 INFO Executor: Finished task 0.0 in stage 172.0 (TID 185). 4127 bytes result sent to driver\n",
      "23/12/12 15:57:31 INFO TaskSetManager: Finished task 0.0 in stage 172.0 (TID 185) in 5 ms on 53aa7a5d52e0 (executor driver) (1/1)\n",
      "23/12/12 15:57:31 INFO TaskSchedulerImpl: Removed TaskSet 172.0, whose tasks have all completed, from pool \n",
      "23/12/12 15:57:31 INFO DAGScheduler: ResultStage 172 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.008 s\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Job 66 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/12/12 15:57:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 172: Stage finished\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Job 66 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.009320 s\n",
      "23/12/12 15:57:31 INFO CodeGenerator: Code generated in 3.728675 ms\n",
      "23/12/12 15:57:31 INFO MemoryStore: Block broadcast_79 stored as values in memory (estimated size 1026.8 KiB, free 29.8 GiB)\n",
      "23/12/12 15:57:31 INFO MemoryStore: Block broadcast_79_piece0 stored as bytes in memory (estimated size 1231.0 B, free 29.8 GiB)\n",
      "23/12/12 15:57:31 INFO BlockManagerInfo: Added broadcast_79_piece0 in memory on 53aa7a5d52e0:36993 (size: 1231.0 B, free: 29.8 GiB)\n",
      "23/12/12 15:57:31 INFO SparkContext: Created broadcast 79 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Registering RDD 243 (showString at NativeMethodAccessorImpl.java:0) as input to shuffle 51\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Got map stage job 67 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Final stage: ShuffleMapStage 174 (showString at NativeMethodAccessorImpl.java:0)\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 173)\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Missing parents: List()\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Submitting ShuffleMapStage 174 (MapPartitionsRDD[243] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/12/12 15:57:31 INFO MemoryStore: Block broadcast_80 stored as values in memory (estimated size 14.6 KiB, free 29.8 GiB)\n",
      "23/12/12 15:57:31 INFO MemoryStore: Block broadcast_80_piece0 stored as bytes in memory (estimated size 7.2 KiB, free 29.8 GiB)\n",
      "23/12/12 15:57:31 INFO BlockManagerInfo: Added broadcast_80_piece0 in memory on 53aa7a5d52e0:36993 (size: 7.2 KiB, free: 29.8 GiB)\n",
      "23/12/12 15:57:31 INFO SparkContext: Created broadcast 80 from broadcast at DAGScheduler.scala:1580\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 174 (MapPartitionsRDD[243] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "23/12/12 15:57:31 INFO TaskSchedulerImpl: Adding task set 174.0 with 1 tasks resource profile 0\n",
      "23/12/12 15:57:31 INFO TaskSetManager: Starting task 0.0 in stage 174.0 (TID 186) (53aa7a5d52e0, executor driver, partition 0, NODE_LOCAL, 7803 bytes) \n",
      "23/12/12 15:57:31 INFO Executor: Running task 0.0 in stage 174.0 (TID 186)\n",
      "23/12/12 15:57:31 INFO ShuffleBlockFetcherIterator: Getting 1 (14.8 KiB) non-empty blocks including 1 (14.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "23/12/12 15:57:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "23/12/12 15:57:31 INFO ShufflePartitionsUtil: For shuffle(43), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "23/12/12 15:57:31 INFO ShufflePartitionsUtil: For shuffle(49), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "23/12/12 15:57:31 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "23/12/12 15:57:31 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Got job 68 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Final stage: ResultStage 176 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 175)\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Missing parents: List()\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Submitting ResultStage 176 (MapPartitionsRDD[246] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
      "23/12/12 15:57:31 INFO MemoryStore: Block broadcast_81 stored as values in memory (estimated size 8.3 KiB, free 29.8 GiB)\n",
      "23/12/12 15:57:31 INFO MemoryStore: Block broadcast_81_piece0 stored as bytes in memory (estimated size 4.2 KiB, free 29.8 GiB)\n",
      "23/12/12 15:57:31 INFO BlockManagerInfo: Added broadcast_81_piece0 in memory on 53aa7a5d52e0:36993 (size: 4.2 KiB, free: 29.8 GiB)\n",
      "23/12/12 15:57:31 INFO SparkContext: Created broadcast 81 from broadcast at DAGScheduler.scala:1580\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 176 (MapPartitionsRDD[246] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
      "23/12/12 15:57:31 INFO TaskSchedulerImpl: Adding task set 176.0 with 1 tasks resource profile 0\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Got job 69 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Final stage: ResultStage 178 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 177)\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Missing parents: List()\n",
      "23/12/12 15:57:31 INFO TaskSetManager: Starting task 0.0 in stage 176.0 (TID 187) (53aa7a5d52e0, executor driver, partition 0, NODE_LOCAL, 7814 bytes) \n",
      "23/12/12 15:57:31 INFO DAGScheduler: Submitting ResultStage 178 (MapPartitionsRDD[247] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
      "23/12/12 15:57:31 INFO Executor: Running task 0.0 in stage 176.0 (TID 187)\n",
      "23/12/12 15:57:31 INFO MemoryStore: Block broadcast_82 stored as values in memory (estimated size 8.3 KiB, free 29.8 GiB)\n",
      "23/12/12 15:57:31 INFO MemoryStore: Block broadcast_82_piece0 stored as bytes in memory (estimated size 4.2 KiB, free 29.8 GiB)\n",
      "23/12/12 15:57:31 INFO BlockManagerInfo: Added broadcast_82_piece0 in memory on 53aa7a5d52e0:36993 (size: 4.2 KiB, free: 29.8 GiB)\n",
      "23/12/12 15:57:31 INFO SparkContext: Created broadcast 82 from broadcast at DAGScheduler.scala:1580\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 178 (MapPartitionsRDD[247] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
      "23/12/12 15:57:31 INFO TaskSchedulerImpl: Adding task set 178.0 with 1 tasks resource profile 0\n",
      "23/12/12 15:57:31 INFO TaskSetManager: Starting task 0.0 in stage 178.0 (TID 188) (53aa7a5d52e0, executor driver, partition 0, NODE_LOCAL, 7814 bytes) \n",
      "23/12/12 15:57:31 INFO ShuffleBlockFetcherIterator: Getting 1 (662.3 KiB) non-empty blocks including 1 (662.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "23/12/12 15:57:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "23/12/12 15:57:31 INFO Executor: Running task 0.0 in stage 178.0 (TID 188)\n",
      "23/12/12 15:57:31 INFO ShuffleBlockFetcherIterator: Getting 1 (157.1 KiB) non-empty blocks including 1 (157.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "23/12/12 15:57:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "23/12/12 15:57:31 INFO Executor: Finished task 0.0 in stage 174.0 (TID 186). 4299 bytes result sent to driver\n",
      "23/12/12 15:57:31 INFO TaskSetManager: Finished task 0.0 in stage 174.0 (TID 186) in 25 ms on 53aa7a5d52e0 (executor driver) (1/1)\n",
      "23/12/12 15:57:31 INFO TaskSchedulerImpl: Removed TaskSet 174.0, whose tasks have all completed, from pool \n",
      "23/12/12 15:57:31 INFO DAGScheduler: ShuffleMapStage 174 (showString at NativeMethodAccessorImpl.java:0) finished in 0.028 s\n",
      "23/12/12 15:57:31 INFO DAGScheduler: looking for newly runnable stages\n",
      "23/12/12 15:57:31 INFO DAGScheduler: running: Set(ShuffleMapStage 165, ShuffleMapStage 166, ResultStage 176, ShuffleMapStage 162, ShuffleMapStage 163, ResultStage 178, ShuffleMapStage 164)\n",
      "23/12/12 15:57:31 INFO DAGScheduler: waiting: Set()\n",
      "23/12/12 15:57:31 INFO DAGScheduler: failed: Set()\n",
      "23/12/12 15:57:31 INFO Executor: Finished task 0.0 in stage 178.0 (TID 188). 124580 bytes result sent to driver\n",
      "23/12/12 15:57:31 INFO TaskSetManager: Finished task 0.0 in stage 178.0 (TID 188) in 9 ms on 53aa7a5d52e0 (executor driver) (1/1)\n",
      "23/12/12 15:57:31 INFO TaskSchedulerImpl: Removed TaskSet 178.0, whose tasks have all completed, from pool \n",
      "23/12/12 15:57:31 INFO DAGScheduler: ResultStage 178 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.011 s\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Job 69 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/12/12 15:57:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 178: Stage finished\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Job 69 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.014191 s\n",
      "23/12/12 15:57:31 INFO MemoryStore: Block broadcast_83 stored as values in memory (estimated size 1149.7 KiB, free 29.8 GiB)\n",
      "23/12/12 15:57:31 INFO Executor: Finished task 0.0 in stage 176.0 (TID 187). 646411 bytes result sent to driver\n",
      "23/12/12 15:57:31 INFO TaskSetManager: Finished task 0.0 in stage 176.0 (TID 187) in 19 ms on 53aa7a5d52e0 (executor driver) (1/1)\n",
      "23/12/12 15:57:31 INFO TaskSchedulerImpl: Removed TaskSet 176.0, whose tasks have all completed, from pool \n",
      "23/12/12 15:57:31 INFO DAGScheduler: ResultStage 176 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.021 s\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Job 68 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/12/12 15:57:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 176: Stage finished\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Job 68 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.022478 s\n",
      "23/12/12 15:57:31 INFO MemoryStore: Block broadcast_83_piece0 stored as bytes in memory (estimated size 180.3 KiB, free 29.8 GiB)\n",
      "23/12/12 15:57:31 INFO BlockManagerInfo: Added broadcast_83_piece0 in memory on 53aa7a5d52e0:36993 (size: 180.3 KiB, free: 29.8 GiB)\n",
      "23/12/12 15:57:31 INFO SparkContext: Created broadcast 83 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "23/12/12 15:57:31 INFO MemoryStore: Block broadcast_84 stored as values in memory (estimated size 4.0 MiB, free 29.8 GiB)\n",
      "23/12/12 15:57:31 INFO MemoryStore: Block broadcast_84_piece0 stored as bytes in memory (estimated size 863.2 KiB, free 29.8 GiB)\n",
      "23/12/12 15:57:31 INFO BlockManagerInfo: Added broadcast_84_piece0 in memory on 53aa7a5d52e0:36993 (size: 863.2 KiB, free: 29.8 GiB)\n",
      "23/12/12 15:57:31 INFO SparkContext: Created broadcast 84 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "23/12/12 15:57:31 INFO ShufflePartitionsUtil: For shuffle(51), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "23/12/12 15:57:31 INFO CodeGenerator: Code generated in 3.651361 ms\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Registering RDD 250 (showString at NativeMethodAccessorImpl.java:0) as input to shuffle 52\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Got map stage job 70 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Final stage: ShuffleMapStage 181 (showString at NativeMethodAccessorImpl.java:0)\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 180)\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Missing parents: List()\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Submitting ShuffleMapStage 181 (MapPartitionsRDD[250] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/12/12 15:57:31 INFO MemoryStore: Block broadcast_85 stored as values in memory (estimated size 14.9 KiB, free 29.8 GiB)\n",
      "23/12/12 15:57:31 INFO MemoryStore: Block broadcast_85_piece0 stored as bytes in memory (estimated size 7.4 KiB, free 29.8 GiB)\n",
      "23/12/12 15:57:31 INFO BlockManagerInfo: Added broadcast_85_piece0 in memory on 53aa7a5d52e0:36993 (size: 7.4 KiB, free: 29.8 GiB)\n",
      "23/12/12 15:57:31 INFO SparkContext: Created broadcast 85 from broadcast at DAGScheduler.scala:1580\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 181 (MapPartitionsRDD[250] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "23/12/12 15:57:31 INFO TaskSchedulerImpl: Adding task set 181.0 with 1 tasks resource profile 0\n",
      "23/12/12 15:57:31 INFO TaskSetManager: Starting task 0.0 in stage 181.0 (TID 189) (53aa7a5d52e0, executor driver, partition 0, NODE_LOCAL, 7803 bytes) \n",
      "23/12/12 15:57:31 INFO Executor: Running task 0.0 in stage 181.0 (TID 189)\n",
      "23/12/12 15:57:31 INFO ShuffleBlockFetcherIterator: Getting 1 (21.7 KiB) non-empty blocks including 1 (21.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "23/12/12 15:57:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "23/12/12 15:57:31 INFO Executor: Finished task 0.0 in stage 181.0 (TID 189). 4299 bytes result sent to driver\n",
      "23/12/12 15:57:31 INFO TaskSetManager: Finished task 0.0 in stage 181.0 (TID 189) in 55 ms on 53aa7a5d52e0 (executor driver) (1/1)\n",
      "23/12/12 15:57:31 INFO TaskSchedulerImpl: Removed TaskSet 181.0, whose tasks have all completed, from pool \n",
      "23/12/12 15:57:31 INFO DAGScheduler: ShuffleMapStage 181 (showString at NativeMethodAccessorImpl.java:0) finished in 0.059 s\n",
      "23/12/12 15:57:31 INFO DAGScheduler: looking for newly runnable stages\n",
      "23/12/12 15:57:31 INFO DAGScheduler: running: Set(ShuffleMapStage 165, ShuffleMapStage 166, ShuffleMapStage 162, ShuffleMapStage 163, ShuffleMapStage 164)\n",
      "23/12/12 15:57:31 INFO DAGScheduler: waiting: Set()\n",
      "23/12/12 15:57:31 INFO DAGScheduler: failed: Set()\n",
      "23/12/12 15:57:31 INFO ShufflePartitionsUtil: For shuffle(52), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "23/12/12 15:57:31 INFO BlockManagerInfo: Removed broadcast_76_piece0 on 53aa7a5d52e0:36993 in memory (size: 4.2 KiB, free: 29.8 GiB)\n",
      "23/12/12 15:57:31 INFO BlockManagerInfo: Removed broadcast_64_piece0 on 53aa7a5d52e0:36993 in memory (size: 8.0 KiB, free: 29.8 GiB)\n",
      "23/12/12 15:57:31 INFO BlockManagerInfo: Removed broadcast_81_piece0 on 53aa7a5d52e0:36993 in memory (size: 4.2 KiB, free: 29.8 GiB)\n",
      "23/12/12 15:57:31 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "23/12/12 15:57:31 INFO BlockManagerInfo: Removed broadcast_62_piece0 on 53aa7a5d52e0:36993 in memory (size: 42.6 KiB, free: 29.8 GiB)\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Got job 71 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Final stage: ResultStage 185 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 184)\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Missing parents: List()\n",
      "23/12/12 15:57:31 INFO BlockManagerInfo: Removed broadcast_67_piece0 on 53aa7a5d52e0:36993 in memory (size: 7.5 KiB, free: 29.8 GiB)\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Submitting ResultStage 185 (MapPartitionsRDD[252] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
      "23/12/12 15:57:31 INFO BlockManagerInfo: Removed broadcast_74_piece0 on 53aa7a5d52e0:36993 in memory (size: 7.5 KiB, free: 29.8 GiB)\n",
      "23/12/12 15:57:31 INFO MemoryStore: Block broadcast_86 stored as values in memory (estimated size 8.3 KiB, free 29.8 GiB)\n",
      "23/12/12 15:57:31 INFO BlockManagerInfo: Removed broadcast_66_piece0 on 53aa7a5d52e0:36993 in memory (size: 7.3 KiB, free: 29.8 GiB)\n",
      "23/12/12 15:57:31 INFO BlockManagerInfo: Removed broadcast_65_piece0 on 53aa7a5d52e0:36993 in memory (size: 6.4 KiB, free: 29.8 GiB)\n",
      "23/12/12 15:57:31 INFO MemoryStore: Block broadcast_86_piece0 stored as bytes in memory (estimated size 4.2 KiB, free 29.8 GiB)\n",
      "23/12/12 15:57:31 INFO BlockManagerInfo: Added broadcast_86_piece0 in memory on 53aa7a5d52e0:36993 (size: 4.2 KiB, free: 29.8 GiB)\n",
      "23/12/12 15:57:31 INFO SparkContext: Created broadcast 86 from broadcast at DAGScheduler.scala:1580\n",
      "23/12/12 15:57:31 INFO BlockManagerInfo: Removed broadcast_85_piece0 on 53aa7a5d52e0:36993 in memory (size: 7.4 KiB, free: 29.8 GiB)\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 185 (MapPartitionsRDD[252] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
      "23/12/12 15:57:31 INFO TaskSchedulerImpl: Adding task set 185.0 with 1 tasks resource profile 0\n",
      "23/12/12 15:57:31 INFO BlockManagerInfo: Removed broadcast_80_piece0 on 53aa7a5d52e0:36993 in memory (size: 7.2 KiB, free: 29.8 GiB)\n",
      "23/12/12 15:57:31 INFO TaskSetManager: Starting task 0.0 in stage 185.0 (TID 190) (53aa7a5d52e0, executor driver, partition 0, NODE_LOCAL, 7814 bytes) \n",
      "23/12/12 15:57:31 INFO BlockManagerInfo: Removed broadcast_61_piece0 on 53aa7a5d52e0:36993 in memory (size: 47.9 KiB, free: 29.8 GiB)\n",
      "23/12/12 15:57:31 INFO Executor: Running task 0.0 in stage 185.0 (TID 190)\n",
      "23/12/12 15:57:31 INFO BlockManagerInfo: Removed broadcast_78_piece0 on 53aa7a5d52e0:36993 in memory (size: 4.2 KiB, free: 29.8 GiB)\n",
      "23/12/12 15:57:31 INFO BlockManagerInfo: Removed broadcast_75_piece0 on 53aa7a5d52e0:36993 in memory (size: 7.3 KiB, free: 29.8 GiB)\n",
      "23/12/12 15:57:31 INFO BlockManagerInfo: Removed broadcast_82_piece0 on 53aa7a5d52e0:36993 in memory (size: 4.2 KiB, free: 29.8 GiB)\n",
      "23/12/12 15:57:31 INFO BlockManagerInfo: Removed broadcast_68_piece0 on 53aa7a5d52e0:36993 in memory (size: 7.5 KiB, free: 29.8 GiB)\n",
      "23/12/12 15:57:31 INFO ShuffleBlockFetcherIterator: Getting 1 (508.7 KiB) non-empty blocks including 1 (508.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "23/12/12 15:57:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "23/12/12 15:57:31 INFO Executor: Finished task 0.0 in stage 185.0 (TID 190). 534017 bytes result sent to driver\n",
      "23/12/12 15:57:31 INFO TaskSetManager: Finished task 0.0 in stage 185.0 (TID 190) in 22 ms on 53aa7a5d52e0 (executor driver) (1/1)\n",
      "23/12/12 15:57:31 INFO TaskSchedulerImpl: Removed TaskSet 185.0, whose tasks have all completed, from pool \n",
      "23/12/12 15:57:31 INFO DAGScheduler: ResultStage 185 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.026 s\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Job 71 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/12/12 15:57:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 185: Stage finished\n",
      "23/12/12 15:57:31 INFO DAGScheduler: Job 71 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.028610 s\n",
      "23/12/12 15:57:31 INFO MemoryStore: Block broadcast_87 stored as values in memory (estimated size 6.0 MiB, free 29.8 GiB)\n",
      "23/12/12 15:57:31 INFO MemoryStore: Block broadcast_87_piece0 stored as bytes in memory (estimated size 1500.0 KiB, free 29.8 GiB)\n",
      "23/12/12 15:57:31 INFO BlockManagerInfo: Added broadcast_87_piece0 in memory on 53aa7a5d52e0:36993 (size: 1500.0 KiB, free: 29.8 GiB)\n",
      "23/12/12 15:57:31 INFO SparkContext: Created broadcast 87 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "23/12/12 15:57:31 INFO JDBCRDD: closed connection\n",
      "23/12/12 15:57:31 INFO Executor: Finished task 0.0 in stage 163.0 (TID 178). 1989 bytes result sent to driver\n",
      "23/12/12 15:57:31 INFO TaskSetManager: Finished task 0.0 in stage 163.0 (TID 178) in 443 ms on 53aa7a5d52e0 (executor driver) (1/1)\n",
      "23/12/12 15:57:31 INFO TaskSchedulerImpl: Removed TaskSet 163.0, whose tasks have all completed, from pool \n",
      "23/12/12 15:57:31 INFO DAGScheduler: ShuffleMapStage 163 (showString at NativeMethodAccessorImpl.java:0) finished in 0.445 s\n",
      "23/12/12 15:57:31 INFO DAGScheduler: looking for newly runnable stages\n",
      "23/12/12 15:57:31 INFO DAGScheduler: running: Set(ShuffleMapStage 165, ShuffleMapStage 166, ShuffleMapStage 162, ShuffleMapStage 164)\n",
      "23/12/12 15:57:31 INFO DAGScheduler: waiting: Set()\n",
      "23/12/12 15:57:31 INFO DAGScheduler: failed: Set()\n",
      "23/12/12 15:57:32 INFO BlockManagerInfo: Removed broadcast_86_piece0 on 53aa7a5d52e0:36993 in memory (size: 4.2 KiB, free: 29.8 GiB)\n",
      "23/12/12 15:57:36 INFO JDBCRDD: closed connection\n",
      "23/12/12 15:57:36 INFO Executor: Finished task 0.0 in stage 164.0 (TID 179). 1989 bytes result sent to driver\n",
      "23/12/12 15:57:36 INFO TaskSetManager: Finished task 0.0 in stage 164.0 (TID 179) in 5862 ms on 53aa7a5d52e0 (executor driver) (1/1)\n",
      "23/12/12 15:57:36 INFO TaskSchedulerImpl: Removed TaskSet 164.0, whose tasks have all completed, from pool \n",
      "23/12/12 15:57:36 INFO DAGScheduler: ShuffleMapStage 164 (showString at NativeMethodAccessorImpl.java:0) finished in 5.866 s\n",
      "23/12/12 15:57:36 INFO DAGScheduler: looking for newly runnable stages\n",
      "23/12/12 15:57:36 INFO DAGScheduler: running: Set(ShuffleMapStage 165, ShuffleMapStage 166, ShuffleMapStage 162)\n",
      "23/12/12 15:57:36 INFO DAGScheduler: waiting: Set()\n",
      "23/12/12 15:57:36 INFO DAGScheduler: failed: Set()\n",
      "23/12/12 15:57:40 INFO JDBCRDD: closed connection 1][Stage 166:>  (0 + 1) / 1]\n",
      "23/12/12 15:57:40 INFO Executor: Finished task 0.0 in stage 165.0 (TID 180). 1989 bytes result sent to driver\n",
      "23/12/12 15:57:40 INFO TaskSetManager: Finished task 0.0 in stage 165.0 (TID 180) in 9206 ms on 53aa7a5d52e0 (executor driver) (1/1)\n",
      "23/12/12 15:57:40 INFO TaskSchedulerImpl: Removed TaskSet 165.0, whose tasks have all completed, from pool \n",
      "23/12/12 15:57:40 INFO DAGScheduler: ShuffleMapStage 165 (showString at NativeMethodAccessorImpl.java:0) finished in 9.210 s\n",
      "23/12/12 15:57:40 INFO DAGScheduler: looking for newly runnable stages\n",
      "23/12/12 15:57:40 INFO DAGScheduler: running: Set(ShuffleMapStage 166, ShuffleMapStage 162)\n",
      "23/12/12 15:57:40 INFO DAGScheduler: waiting: Set()\n",
      "23/12/12 15:57:40 INFO DAGScheduler: failed: Set()\n",
      "[Stage 162:>                (0 + 1) / 1][Stage 166:>                (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "spark.sparkContext.setLogLevel(\"INFO\")\n",
    "spark.sql(\"SET spark.sql.yannakakis.enabled = false\").show()\n",
    "#spark.sql(\"SET spark.sql.yannakakis.enabled = false\").show()\n",
    "#df = run_query(spark, 'stats-queries/142-135.sql')\n",
    "#df = run_query(spark, 'stats-queries/hints/142-135-hint.sql')\n",
    "#df = run_query(spark, 'stats-queries/hints/141-068-hint.sql')\n",
    "df = run_query(spark, 'lsqb/sql/q1.sql')\n",
    "df.show()\n",
    "print(explain_str(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f6e0861d-f87e-4ab4-8e1d-23d9c6752c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|  174305|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = spark.sql('select count(*) from comments as c')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "34f47b7f-d053-42cf-b50c-cf78aeefe9ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('cache table comments').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae1b92b6-7d05-49b5-aef0-2313a0d9a4cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'b'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1 = 'a'\n",
    "a2 = 'a'\n",
    "\n",
    "set([a1, 'b']) - set([a2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8668da-83dd-46b6-8901-edd97b0eb8e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

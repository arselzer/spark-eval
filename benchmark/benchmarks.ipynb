{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d98322b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "import string\n",
    "import pathlib\n",
    "import random\n",
    "import threading\n",
    "import time\n",
    "from urllib.parse import urlsplit, urlunsplit\n",
    "import requests\n",
    "import json\n",
    "from py4j.protocol import Py4JJavaError, Py4JError\n",
    "import glob\n",
    "import psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a04030-0a23-45df-8e70-6a296a4f582d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global configuration\n",
    "SPARK_MEMORY = 900\n",
    "SPARK_CORES = 60\n",
    "DBHOST = 'postgres'\n",
    "QUERY_TIMEOUT = 60 * 30\n",
    "QUERY_TIMEOUT = 60 * 180"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7397f3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spark():\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"app\") \\\n",
    "        .master(f'local[{SPARK_CORES}]') \\\n",
    "        .config(\"spark.driver.memory\", f'{SPARK_MEMORY}g') \\\n",
    "        .config(\"spark.executor.memory\", f'{SPARK_MEMORY}g') \\\n",
    "        .config(\"spark.memory.offHeap.enabled\",False) \\\n",
    "        .config(\"spark.jars\", \"postgresql-42.3.3.jar\") \\\n",
    "        .getOrCreate()\n",
    "    return spark\n",
    "\n",
    "## Cluster\n",
    "# def create_spark():\n",
    "#     spark = SparkSession.builder \\\n",
    "#         .appName(\"app\") \\\n",
    "#         .master('spark://10.100.42.35:7078') \\\n",
    "#         .config(\"spark.driver.memory\", f'{SPARK_MEMORY}g') \\\n",
    "#         .config(\"spark.executor.memory\", f'{SPARK_MEMORY}g') \\\n",
    "#         .config(\"spark.driver.host\", \"10.100.42.223\") \\\n",
    "#         .config(\"spark.driver.bindAddress\", \"0.0.0.0\") \\\n",
    "#         .config(\"spark.driver.port\", \"4060\") \\\n",
    "#         .config(\"spark.memory.offHeap.enabled\",OFFHEAP) \\\n",
    "#         .config(\"spark.jars\", \"postgresql-42.3.3.jar\") \\\n",
    "#         .getOrCreate()\n",
    "#     return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4170360b-134b-4619-915b-391e877bc203",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_metrics(spark, group_id):\n",
    "    parsed = list(urlsplit(spark.sparkContext.uiWebUrl))\n",
    "    host_port = parsed[1]\n",
    "    parsed[1] = 'localhost' + host_port[host_port.find(':'):]\n",
    "    API_URL = f'{urlunsplit(parsed)}/api/v1'\n",
    "\n",
    "    app_id = spark.sparkContext.applicationId\n",
    "    sql_queries = requests.get(API_URL + f'/applications/{app_id}/sql', params={'length': '100000'}).json()\n",
    "    query_ids = [q['id'] for q in sql_queries if q['description'] == group_id]\n",
    "    if (len(query_ids) == 0):\n",
    "        print(f'query with group {group_id} not found')\n",
    "        return None\n",
    "    query_id = query_ids[0]\n",
    "    print(f'query id: {query_id}')\n",
    "    \n",
    "    query_details = requests.get(API_URL + f'/applications/{app_id}/sql/{query_id}',\n",
    "                                 params={'details': 'true', 'planDescription': 'true'}).json()\n",
    "    \n",
    "    success_job_ids = query_details['successJobIds']\n",
    "    running_job_ids = query_details['runningJobIds']\n",
    "    failed_job_ids = query_details['failedJobIds']\n",
    "    \n",
    "    job_ids = success_job_ids + running_job_ids + failed_job_ids\n",
    "    \n",
    "    job_details = [requests.get(API_URL + f'/applications/{app_id}/jobs/{jid}').json() for jid in job_ids]\n",
    "    \n",
    "    job_stages = {}\n",
    "    \n",
    "    for j in job_details:\n",
    "        stage_ids = j['stageIds']\n",
    "        \n",
    "        stage_params = {'details': 'true', 'withSummaries': 'true'}\n",
    "        stages = [requests.get(API_URL + f'/applications/{app_id}/stages/{sid}', stage_params) for sid in stage_ids]\n",
    "        \n",
    "        job_stages[j['jobId']] = [stage.json() for stage in stages if stage.status_code == 200] # can be 404\n",
    "    \n",
    "    return query_details, job_details, job_stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbbca1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_db(spark, dbname):\n",
    "    \n",
    "    username = dbname\n",
    "    password = dbname\n",
    "    dbname = dbname\n",
    "\n",
    "    df_tables = spark.read.format(\"jdbc\") \\\n",
    "    .option(\"url\", f'jdbc:postgresql://{DBHOST}:5432/{dbname}') \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .option(\"dbtable\", \"information_schema.tables\") \\\n",
    "    .option(\"user\", username) \\\n",
    "    .option(\"password\", password) \\\n",
    "    .load()\n",
    "\n",
    "    for idx, row in df_tables.toPandas().iterrows():\n",
    "        if row.table_schema == 'public':\n",
    "            table_name = row.table_name\n",
    "            df = spark.read.format(\"jdbc\") \\\n",
    "                .option(\"url\", f'jdbc:postgresql://{DBHOST}:5432/{dbname}') \\\n",
    "                .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "                .option(\"dbtable\", table_name) \\\n",
    "                .option(\"user\", username) \\\n",
    "                .option(\"password\", password) \\\n",
    "                .load()\n",
    "    \n",
    "            print(table_name)\n",
    "            #print(df.show())\n",
    "            df.createOrReplaceTempView(table_name)\n",
    "\n",
    "def random_str(size=16, chars=string.ascii_uppercase + string.digits):\n",
    "    return ''.join(random.choice(chars) for _ in range(size))\n",
    "\n",
    "def set_group_id(spark):\n",
    "    group_id = random_str()\n",
    "    spark.sparkContext.setJobGroup(group_id, group_id)\n",
    "    return group_id\n",
    "\n",
    "def cancel_query(spark, seconds, group_id):\n",
    "    time.sleep(seconds)\n",
    "    print(\"cancelling jobs with id \" + group_id)\n",
    "    print(spark.sparkContext.cancelJobGroup(group_id))\n",
    "    print(\"cancelled job\")\n",
    "\n",
    "def cancel_query_after(spark, seconds):\n",
    "    group_id = random_str()\n",
    "    spark.sparkContext.setJobGroup(group_id, group_id)\n",
    "    threading.Thread(target=cancel_query, args=(spark, seconds, group_id,)).start()\n",
    "    return group_id\n",
    "    \n",
    "def run_query(spark, file):\n",
    "    with open(file, 'r') as f:\n",
    "        query = '\\n'.join(filter(lambda line: not line.startswith('limit') and not line.startswith('-'), f.readlines()))\n",
    "        \n",
    "        print(\"running query: \\n\" + query)\n",
    "        return spark.sql(query)\n",
    "\n",
    "def get_resource_usage(t):\n",
    "    return {\n",
    "        'time': t,\n",
    "        'memory': psutil.virtual_memory(),\n",
    "        'cpu': psutil.cpu_percent(interval=None, percpu=True),\n",
    "        'cpu_total': psutil.cpu_percent(interval=None, percpu=False)\n",
    "    }\n",
    "def explain_str(df):\n",
    "    return df._sc._jvm.PythonSQLUtils.explainString(df._jdf.queryExecution(), 'extended')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead489e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "resource_usage = []\n",
    "\n",
    "def measure_resource_usage(resource_usage):\n",
    "    t = threading.current_thread()\n",
    "    secs = 0\n",
    "    while getattr(t, \"do_run\", True):\n",
    "        resource_usage.append(get_resource_usage(secs))\n",
    "        #print(\"resource usage: \" + str(resource_usage))\n",
    "        secs += 1\n",
    "        time.sleep(1)\n",
    "\n",
    "def benchmark_query(spark, query, respath, run):\n",
    "    spark.sparkContext._jvm.System.gc()\n",
    "    start_time = time.time()\n",
    "\n",
    "    resource_usage = []\n",
    "\n",
    "    measure_thread = threading.Thread(target=measure_resource_usage, args=(resource_usage, ))\n",
    "    measure_thread.start()\n",
    "\n",
    "    group_id = cancel_query_after(spark, QUERY_TIMEOUT)\n",
    "    df1 = run_query(spark, query)\n",
    "    df1.show()\n",
    "\n",
    "    measure_thread.do_run = False\n",
    "\n",
    "    end_time = time.time()\n",
    "    diff_time = end_time - start_time\n",
    "\n",
    "    execution, jobs, job_stages = extract_metrics(spark, group_id)\n",
    "\n",
    "    with open(respath + f'/resource-usage-{run}.json', 'w') as f:\n",
    "        f.write(json.dumps(resource_usage, indent=2))\n",
    "    with open(respath + f'/explain-{run}.txt', 'w') as f:\n",
    "        f.write(explain_str(df1))\n",
    "\n",
    "    resource_list = map(lambda r: [r['time'], r['memory'].used, r['cpu_total']], resource_usage)\n",
    "    resource_df = pd.DataFrame(resource_list, columns = ['time', 'memory_used', 'cpu_used'])\n",
    "    resource_df.to_csv(respath + f'/resource-usage-{run}.csv')\n",
    "\n",
    "    peak_memory = max(map(lambda r: r['memory'].used, resource_usage)) / (1000 * 1000 * 1000) # GB\n",
    "\n",
    "    if execution is not None:\n",
    "            with open(respath + f'/execution-{run}.json', 'w') as f:\n",
    "                f.write(json.dumps(execution, indent=2))\n",
    "            with open(respath + f'/jobs-{run}.json', 'w') as f:\n",
    "                f.write(json.dumps(jobs, indent=2))\n",
    "            with open(respath + f'/stages-{run}.json', 'w') as f:\n",
    "                f.write(json.dumps(job_stages, indent=2))\n",
    "    return (diff_time, peak_memory)\n",
    "\n",
    "def benchmark(spark, dbname, query_file, mode, run):\n",
    "    #spark.sql(\"SET spark.sql.yannakakis.enabled = false\").show()\n",
    "    # run the query once to warm up Spark (load the relation in memory)\n",
    "    #df0 = run_query(query)\n",
    "    #df0.show()\n",
    "    \n",
    "    query_name = os.path.basename(query_file)\n",
    "\n",
    "    respath = f'benchmark-results-{dbname}/' + query_name + \"/\" + mode\n",
    "    pathlib.Path(respath).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if mode == \"opt\":\n",
    "        spark.sql(\"SET spark.sql.yannakakis.enabled = true\").show()\n",
    "    elif mode == \"ref\":\n",
    "        spark.sql(\"SET spark.sql.yannakakis.enabled = false\").show()\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        (runtime, peak_memory) = benchmark_query(spark, query_file, respath, run)\n",
    "        return [query_name, runtime, peak_memory, mode, run]\n",
    "    except Py4JError as e:\n",
    "        print('timeout or error: ' + str(e))\n",
    "        return [query_name, None, None, mode, run]\n",
    "\n",
    "def benchmark_all(dbname, mode, runs, queries, group_in_leaves=False, physical_cj=False):\n",
    "    spark = create_spark()\n",
    "    import_db(spark, dbname)\n",
    "\n",
    "    if physical_cj:\n",
    "        spark.sql(\"SET spark.sql.codegen.wholeStage = true\").show()\n",
    "        spark.sql(\"SET spark.sql.yannakakis.physicalCountJoinEnabled = true\").show()\n",
    "    else:\n",
    "        spark.sql(\"SET spark.sql.codegen.wholeStage = true\").show()\n",
    "        spark.sql(\"SET spark.sql.yannakakis.physicalCountJoinEnabled = false\").show()\n",
    "    if group_in_leaves:\n",
    "        spark.sql(\"SET spark.sql.yannakakis.countGroupInLeaves = true\").show()\n",
    "    else:\n",
    "        spark.sql(\"SET spark.sql.yannakakis.countGroupInLeaves = false\").show()\n",
    "\n",
    "    results_df = df = pd.DataFrame([], columns = ['query', 'runtime', 'peak_memory', 'mode', 'run', 'group_leaves', 'physical_cj'])\n",
    "    results_file = f'benchmark-results-{dbname}/results-{mode}.csv'\n",
    "    if (os.path.exists(results_file)):\n",
    "        results_df = pd.read_csv(results_file, index_col=0)\n",
    "\n",
    "    for run in runs:\n",
    "        for q in queries:\n",
    "            results = [benchmark(spark, dbname, q, mode, run) + [group_in_leaves, physical_cj]]\n",
    "            new_df = pd.DataFrame(results, columns = ['query', 'runtime', 'peak_memory', 'mode', 'run', 'group_leaves', 'physical_cj'])\n",
    "            results_df = pd.concat([results_df, new_df], ignore_index=True)\n",
    "            results_df.to_csv(f'benchmark-results-{dbname}/results-{mode}.csv')\n",
    "            print(results_df)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398c3837-5334-46f7-9f4e-b01966e8749a",
   "metadata": {},
   "source": [
    "## SNAP Benchmark\n",
    "\n",
    "### Optimized execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be8c1d5-2fe0-4853-81ae-56f40f8a8097",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### benchmark configuration\n",
    "group_in_leaves = False\n",
    "dbname = 'snap'\n",
    "mode = 'opt'\n",
    "runs = ['1', '2', '3', '4', '5', '6']\n",
    "#runs = ['1']\n",
    "####\n",
    "\n",
    "tables = ['patents', 'wiki', 'google', 'dblp']\n",
    "#tables = ['wiki']\n",
    "\n",
    "\n",
    "for tablename in tables:\n",
    "    queries = sorted(glob.glob(f'snap-queries/all/{tablename}-*'))\n",
    "    print('running queries: ' + str(queries))\n",
    "    benchmark_all(dbname, mode, runs, queries, physical_cj=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0e5d6e-463b-4fe5-afec-a01bef351ce4",
   "metadata": {},
   "source": [
    "### Ref execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4c01dd-4cd3-4cb7-8e44-58f9deb2327a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### benchmark configuration\n",
    "group_in_leaves = False\n",
    "dbname = 'snap'\n",
    "mode = 'ref'\n",
    "runs = ['1', '2', '3', '4', '5', '6']\n",
    "####\n",
    "\n",
    "queries = ['snap-queries/all/patents-path02.sql',\n",
    "          'snap-queries/all/patents-path03.sql',\n",
    "          'snap-queries/all/patents-path04.sql',\n",
    "          'snap-queries/all/patents-path05.sql',\n",
    "          'snap-queries/all/patents-tree01.sql',\n",
    "          'snap-queries/all/wiki-path02.sql',\n",
    "           'snap-queries/all/google-path02.sql',\n",
    "           'snap-queries/all/google-path03.sql',\n",
    "           'snap-queries/all/google-path04.sql',\n",
    "           'snap-queries/all/dblp-path02.sql',\n",
    "           'snap-queries/all/dblp-path03.sql',\n",
    "           'snap-queries/all/dblp-path04.sql',\n",
    "           'snap-queries/all/dblp-path05.sql',\n",
    "           'snap-queries/all/dblp-tree01.sql',\n",
    "           'snap-queries/all/dblp-tree02.sql'\n",
    "          ]\n",
    "\n",
    "\n",
    "print('running queries: ' + str(queries))\n",
    "\n",
    "benchmark_all(dbname, mode, runs, queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a937d945-3c03-494f-8a5f-8f1f69bf5a22",
   "metadata": {},
   "source": [
    "## LSQB Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7beb471-30e3-41c8-a6ba-4aac25bc4102",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### benchmark configuration\n",
    "dbname = 'lsqb'\n",
    "group_in_leaves = False\n",
    "physical_cj = True\n",
    "#mode = 'opt'\n",
    "runs = ['1', '2', '3', '4', '5', '6']\n",
    "runs = ['1', '2']\n",
    "####\n",
    "\n",
    "queries = ['lsqb/sql/q1.sql', 'lsqb/sql/q4.sql']\n",
    "queries_hints = ['lsqb/sql/q1-hint.sql', 'lsqb/sql/q4-hint.sql']\n",
    "\n",
    "print('running queries: ' + str(queries))\n",
    "#benchmark_all(dbname, 'opt', runs, queries, group_in_leaves=False, physical_cj=True)\n",
    "#benchmark_all(dbname, 'opt', runs, queries_hints, group_in_leaves=False, physical_cj=True)\n",
    "\n",
    "#benchmark_all(dbname, 'opt', ['1'], ['lsqb/sql/q4.sql', 'lsqb/sql/q4-hint.sql'], group_in_leaves=False, physical_cj=True)\n",
    "#benchmark_all(dbname, 'opt', ['3', '4', '5', '6'], ['lsqb/sql/q4.sql', 'lsqb/sql/q4-hint.sql'], group_in_leaves=False, physical_cj=True)\n",
    "#benchmark_all(dbname, 'opt', ['1', '2', '3', '4', '5', '6'], ['lsqb/sql/q4.sql', 'lsqb/sql/q4-hint.sql'], group_in_leaves=False, physical_cj=False)\n",
    "\n",
    "#benchmark_all(dbname, 'opt', ['1', '2', '3', '4', '5', '6'], ['lsqb/sql/q1-hint.sql'], group_in_leaves=False, physical_cj=False)\n",
    "#benchmark_all(dbname, 'opt', ['1', '2', '3', '4', '5', '6'], ['lsqb/sql/q1-hint.sql'], group_in_leaves=False, physical_cj=True)\n",
    "#benchmark_all(dbname, 'opt', ['3', '4', '5', '6'], ['lsqb/sql/q1.sql'], group_in_leaves=False, physical_cj=False)\n",
    "#benchmark_all(dbname, 'opt', ['3', '4', '5', '6'], ['lsqb/sql/q1.sql'], group_in_leaves=False, physical_cj=True)\n",
    "benchmark_all(dbname, 'ref', ['4', '5', '6'], ['lsqb/sql/q1.sql'])\n",
    "\n",
    "\n",
    "#benchmark_all(dbname, 'opt', runs, queries_hints, group_in_leaves=False, physical_cj=True)\n",
    "#benchmark_all(dbname, 'ref', ['3', '4', '5', '6'], ['lsqb/sql/q4.sql'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09492e05",
   "metadata": {},
   "source": [
    "## TPC-H Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67544c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### benchmark configuration\n",
    "group_in_leaves = False\n",
    "dbname = 'tpch'\n",
    "runs = ['1', '2', '3', '4', '5', '6']\n",
    "#runs = ['x1', 'x2']\n",
    "####\n",
    "\n",
    "queries = ['tpch-kit/dbgen/queries/postgres/2.sql',\n",
    "           'tpch-kit/dbgen/queries/postgres/11.sql', \n",
    "           'tpch-kit/dbgen/queries/postgres/11-hint.sql']\n",
    "queries += ['tpch-queries/median-1.sql', 'tpch-queries/median-1-hint.sql']\n",
    "#queries = ['tpch-queries/median-1.sql', 'tpch-queries/median-1-hint.sql' ]\n",
    "#queries = ['tpch-kit/dbgen/queries/postgres/11.sql', \n",
    "#           'tpch-kit/dbgen/queries/postgres/11-hint.sql']\n",
    "#queries = ['tpch-queries/2-subq.sql'] #, 'tpch-queries/2-subq-hint.sql']\n",
    "\n",
    "print('running queries: ' + str(queries))\n",
    "benchmark_all(dbname, 'ref', ['3', '4', '5', '6'], queries)\n",
    "benchmark_all(dbname, 'opt', ['3', '4', '5', '6'], queries, group_in_leaves = group_in_leaves, physical_cj=True)\n",
    "benchmark_all(dbname, 'opt', ['1', '2', '3', '4', '5', '6'], queries, group_in_leaves = group_in_leaves, physical_cj=False)\n",
    "#benchmark_all(dbname, 'opt', runs, queries, group_in_leaves = group_in_leaves, physical_cj=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380628a7-8b9f-40b6-b2ba-c4089897e84f",
   "metadata": {},
   "source": [
    "## JOB (IMDB) Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251fb301-811b-49ab-ada6-8f2b5d566031",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### benchmark configuration\n",
    "group_in_leaves = False\n",
    "dbname = 'imdb'\n",
    "runs = ['1', '2', '3', '4', '5', '6']\n",
    "####\n",
    "\n",
    "queries = ['job/2a.sql', 'job/2b.sql', 'job/2c.sql', 'job/2d.sql',\n",
    "           'job/3a.sql', 'job/3b.sql', 'job/3c.sql',\n",
    "           'job/5a.sql', 'job/5b.sql', 'job/5c.sql',\n",
    "           'job/17a.sql', 'job/17b.sql', 'job/17c.sql', 'job/17d.sql', 'job/17e.sql', 'job/17f.sql',\n",
    "           'job/20a.sql', 'job/20b.sql', 'job/20c.sql',\n",
    "          ]\n",
    "\n",
    "print('running queries: ' + str(queries))\n",
    "benchmark_all(dbname, 'opt', runs, queries, physical_cj=True)\n",
    "#benchmark_all(dbname, 'ref', runs, queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c1c9eb-9e98-43cd-9cdb-e6910759b78f",
   "metadata": {},
   "source": [
    "## STATS Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18dc713f-1a8a-473d-ae10-b126963873f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### benchmark configuration\n",
    "dbname = 'stats'\n",
    "#mode = 'opt'\n",
    "runs = ['1', '2', '3', '4', '5', '6']\n",
    "#runs = ['04']\n",
    "#runs = ['01']\n",
    "####\n",
    "\n",
    "queries = sorted(glob.glob('stats-queries/*.sql'))\n",
    "queries_hint = sorted(glob.glob('stats-queries/hints/*.sql'))\n",
    "\n",
    "print('running queries: ' + str(queries))\n",
    "#benchmark_all(dbname, 'opt', runs, queries, physical_cj=True)\n",
    "benchmark_all(dbname, 'opt', runs, queries_hint, group_in_leaves=True, physical_cj=True)\n",
    "#benchmark_all(dbname, 'ref', runs, queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df9b0bf-8eb5-44fb-9252-a9386ca5bd8c",
   "metadata": {},
   "source": [
    "## hetionet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d07366a-130f-43a8-8b8b-4c0a28afde21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### benchmark configuration\n",
    "dbname = 'hetio'\n",
    "#mode = 'opt'\n",
    "runs = ['1', '2', '3', '4', '5', '6']\n",
    "#runs = ['04']\n",
    "#runs = ['01']\n",
    "####\n",
    "\n",
    "queries = sorted(glob.glob('hetio/*.sql'))\n",
    "#queries = ['hetio/CtDpSpD.sql']\n",
    "\n",
    "print('running queries: ' + str(queries))\n",
    "#benchmark_all(dbname, 'opt', runs, queries, physical_cj=True)\n",
    "benchmark_all(dbname, 'opt', runs, queries, group_in_leaves=False, physical_cj=True)\n",
    "benchmark_all(dbname, 'opt', runs, queries, group_in_leaves=False, physical_cj=False)\n",
    "benchmark_all(dbname, 'ref', runs, queries)\n",
    "#benchmark_all(dbname, 'ref', runs, queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08624383-3a60-402d-9a3d-3d5bd44b9a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = create_spark()\n",
    "#import_db(spark, 'stats')\n",
    "import_db(spark, 'lsqb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea49cfe-1fa5-41d2-b621-0d9a8b5a3958",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.setLogLevel(\"INFO\")\n",
    "spark.sql(\"SET spark.sql.yannakakis.enabled = false\").show()\n",
    "#spark.sql(\"SET spark.sql.yannakakis.enabled = false\").show()\n",
    "#df = run_query(spark, 'stats-queries/142-135.sql')\n",
    "#df = run_query(spark, 'stats-queries/hints/142-135-hint.sql')\n",
    "#df = run_query(spark, 'stats-queries/hints/141-068-hint.sql')\n",
    "df = run_query(spark, 'lsqb/sql/q1.sql')\n",
    "df.show()\n",
    "print(explain_str(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e0861d-f87e-4ab4-8e1d-23d9c6752c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = spark.sql('select count(*) from comments as c')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f47b7f-d053-42cf-b50c-cf78aeefe9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('cache table comments').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1b92b6-7d05-49b5-aef0-2313a0d9a4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = 'a'\n",
    "a2 = 'a'\n",
    "\n",
    "set([a1, 'b']) - set([a2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8668da-83dd-46b6-8901-edd97b0eb8e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
